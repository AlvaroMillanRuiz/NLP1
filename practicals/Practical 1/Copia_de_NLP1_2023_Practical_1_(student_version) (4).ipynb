{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002).\n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused.\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**.\n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaZnxptMJiD7",
        "outputId": "a8645f9c-c294-4b70-b981-41dc2a026f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.8\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download).\n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hok-BFu9lGoK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "#from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "penenenenennee\n"
          ]
        }
      ],
      "source": [
        "print('penenenenennee')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careEKj-mRpl",
        "outputId": "81cffc80-c92e-407d-8ff4-cc283a99f713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "5 NEG 35\n",
            "In/IN 2176/CD on/IN the/DT planet/NN Mars/NNP police/NN taking/VBG into/IN custody/NN an/DT accused/VBN murderer/NN face/VBP the/DT title/NN menace/NN ./.\n",
            "6 NEG 27\n",
            "So/RB ask/VB yourself/PRP what/WP ``/`` 8MM/NN ''/'' -LRB-/-LRB- ``/`` Eight/CD Millimeter/NNP ''/'' -RRB-/-RRB- is/VBZ really/RB all/DT about/RB ./.\n",
            "7 NEG 25\n",
            "That/DT 's/VBZ exactly/RB how/WRB long/RB the/DT movie/NN felt/VBD to/TO me/PRP ./.\n",
            "8 NEG 33\n",
            "Call/VB it/PRP a/DT road/NN trip/NN for/IN the/DT walking/VBG wounded/VBN ./.\n",
            "9 NEG 42\n",
            "A/DT young/JJ French/JJ boy/NN sees/VBZ his/PRP$ parents/NNS killed/VBN before/IN his/PRP$ eyes/NNS by/IN Tim/NNP Roth/NNP ,/, oops/NNS .../: I/PRP mean/VBP ,/, an/DT evil/JJ man/NN ./.\n",
            "10 NEG 33\n",
            "Best/NN remembered/VBD for/IN his/PRP$ understated/VBN performance/NN as/IN Dr./NNP Hannibal/NNP Lecter/NNP in/IN Michael/NNP Mann/NNP 's/POS forensics/NNS thriller/NN ,/, Manhunter/NNP ,/, Scottish/NNP character/NN actor/NN Brian/NNP Cox/NNP brings/VBZ something/NN special/JJ to/TO every/DT movie/NN he/PRP works/VBZ on/IN ./.\n",
            "11 NEG 26\n",
            "Janeane/NNP Garofalo/NNP in/IN a/DT romantic/JJ comedy/NN --/: it/PRP was/VBD a/DT good/JJ idea/NN a/DT couple/NN years/NNS ago/IN with/IN THE/DT TRUTH/NN ABOUT/IN CATS/NNS AND/CC DOGS/NNS but/CC is/VBZ almost/RB excruciating/JJ in/IN THE/DT MATCHMAKER/NN ./.\n",
            "12 NEG 18\n",
            "And/CC now/RB the/DT high-flying/JJ Hong/NNP Kong/NNP style/NN of/IN filmmaking/NN has/VBZ made/VBN its/PRP$ way/NN down/IN to/TO the/DT classics/NNS ,/, and/CC it/PRP is/VBZ n't/RB pretty/RB ./.\n",
            "13 NEG 40\n",
            "A/DT movie/NN like/IN MORTAL/JJ KOMBAT/NN :/: ANNIHILATION/NN works/NNS -LRB-/-LRB- and/CC must/MD be/VB reviewed/VBN on/IN -RRB-/-RRB- multiple/JJ levels/NNS ./.\n",
            "14 NEG 24\n",
            "She/PRP was/VBD the/DT femme/NN in/IN ``/`` La/NNP Femme/NNP Nikita/NNP ./. ''/''\n",
            "15 NEG 30\n",
            "John/NNP Carpenter/NNP makes/VBZ B-movies/NNS ./.\n",
            "16 NEG 34\n",
            "I/PRP 'm/VBP really/RB starting/VBG to/TO wonder/VB about/IN Alicia/NNP Silverstone/NNP ./.\n",
            "17 NEG 30\n",
            "So/RB what/WP do/VBP you/PRP get/VB when/WRB you/PRP mix/VBP together/RB plot/NN elements/NNS from/IN various/JJ successful/JJ sci-fi/JJ films/NNS such/JJ as/IN CLOSE/NNP ENCOUNTERS/NNPS OF/IN THE/DT THIRD/JJ KIND/NN ,/, 2001/CD :/: A/DT SPACE/NN ODYSSEY/NN ,/, APOLLO/NNP 13/CD and/CC CONTACT/NNP ?/.\n",
            "18 NEG 23\n",
            "The/DT Law/NN of/IN Crowd/NN Pleasing/NNP Romantic/JJ Movies/NNS states/VBZ that/IN the/DT two/CD leads/NNS must/MD end/VB up/RP together/RB by/IN film/NN 's/POS end/NN ./.\n",
            "19 NEG 40\n",
            "Mighty/NNP Joe/NNP Young/NNP blunders/NNS about/IN for/IN nearly/RB twenty/CD minutes/NNS before/IN we/PRP actually/RB get/VBP to/TO see/VB a/DT great/JJ big/JJ gorilla/NN ./.\n",
            "20 NEG 33\n",
            "``/`` Spawn/VB ''/'' features/VBZ good/JJ guys/NNS ,/, bad/JJ guys/NNS ,/, lots/NNS of/IN fighting/NN ,/, bloody/JJ violence/NN ,/, a/DT leather-clad/JJ machine/NN gun/NN chick/NN ,/, gooey/NN ,/, self-healing/JJ bullet/NN holes/NNS ,/, scatological/JJ humor/NN and/CC a/DT man-eating/JJ monster/NN ./.\n",
            "21 NEG 26\n",
            "IN/IN DREAMS/NNS ``/`` In/IN Dreams/NNS ''/'' might/MD keep/VB you/PRP awake/RB at/IN night/NN ,/, but/CC not/RB because/IN of/IN its/PRP$ creepy/JJ imagery/NN ,/, bizarre/JJ visual/JJ style/NN or/CC story/NN about/IN a/DT clairvoyant/JJ madman/NN who/WP lures/VBZ young/JJ girls/NNS to/TO their/PRP$ untimely/JJ deaths/NNS ./.\n",
            "22 NEG 41\n",
            "Starring/VBG Jean-Claude/NNP Van/NNP Damme/NNP ,/, Rob/NNP Schneider/NNP ,/, Lela/NNP Rochon/NNP ,/, Paul/NNP Sorvino/NNP ``/`` Knock/VB Off/IN ''/'' is/VBZ exactly/RB that/IN :/: a/DT cheap/JJ knock/VB off/IN of/IN an/DT action/NN movie/NN ./.\n",
            "23 NEG 34\n",
            "``/`` Snake/NNP Eyes/NNS ''/'' is/VBZ the/DT most/RBS aggravating/JJ kind/NN of/IN movie/NN :/: the/DT kind/NN that/WDT shows/VBZ so/RB much/JJ potential/JJ then/RB becomes/VBZ unbelievably/RB disappointing/JJ ./.\n",
            "24 NEG 34\n",
            "Forgive/VB the/DT fevered/JJ criticism/NN but/CC the/DT fervor/NN of/IN THE/DT CRUCIBLE/NN infects/VBZ ./.\n",
            "25 NEG 31\n",
            "One/NN might/MD expect/VB a/DT cathartic/JJ viewing/NN experience/NN walking/VBG into/IN a/DT new/JJ Jean-Luc/NNP Godard/NNP film/NN ./.\n",
            "26 NEG 23\n",
            "America/NNP loves/VBZ convenience/NN ./.\n",
            "27 NEG 37\n",
            "REINDEER/NNP GAMES/NNP is/VBZ easily/RB the/DT worst/JJS of/IN the/DT three/CD recent/JJ films/NNS penned/VBN by/IN Ehren/NNP Kruger/NNP -LRB-/-LRB- SCREAMand/NNP ARLINGTON/NNP RD/NNP are/VBP the/DT others/NNS ,/, each/DT derivative/NN in/IN their/PRP$ own/JJ special/JJ way/NN -RRB-/-RRB- ./.\n",
            "28 NEG 21\n",
            "Cruella/NNP De/NNP Vil/NNP reformed/VBN ?/.\n",
            "29 NEG 20\n",
            "One-sided/JJ ``/`` doom/NN and/CC gloom/NN ''/'' documentary/NN about/IN the/DT possible/JJ annihilation/NN of/IN the/DT human/JJ race/NN as/IN foretold/NN by/IN the/DT Bible/NNP ./.\n",
            "30 NEG 27\n",
            "Play/VB it/PRP to/TO the/DT Bone/NN ,/, the/DT newest/JJS addition/NN to/TO Ron/NNP Shelton/NNP 's/POS sports-themed/JJ repertoire/NN fails/VBZ in/IN numerous/JJ ways/NNS ./.\n",
            "31 NEG 52\n",
            "Some/DT movies/NNS I/PRP should/MD just/RB skip/VB ./.\n",
            "32 NEG 73\n",
            "``/`` ROMEO/NNP MUST/NNP DIE/VB ''/'' Review/NN by/IN R.L./NNP Strong/JJ out/IN of/IN WARNER/NNP BROS./NNP ./.\n",
            "33 NEG 17\n",
            "I/PRP wish/VBP I/PRP could/MD have/VB been/VBN in/IN the/DT pitch/NN meeting/NN for/IN this/DT ridiculous/JJ notion/NN of/IN a/DT sports/NNS film/NN ./.\n",
            "34 NEG 23\n",
            "If/IN you/PRP 're/VBP into/IN watching/VBG near/IN on/IN two/CD hours/NNS of/IN bored/JJ ,/, foul-mouthed/JJ Florida/NNP teens/NNS having/VBG sex/NN ,/, doing/VBG drugs/NNS ,/, having/VBG sex/NN ,/, listening/VBG to/TO Eminem/NNP ,/, having/VBG sex/NN ,/, playing/VBG video/NN games/NNS ,/, having/VBG sex/NN ,/, and/CC killing/VBG one/CD of/IN their/PRP$ peers/NNS ,/, then/RB ``/`` Bully/NNP ''/'' 's/POS for/IN you/PRP ./.\n",
            "35 NEG 28\n",
            "Sean/NNP Connery/NNP stars/VBZ as/IN a/DT Harvard/NNP law/NN professor/NN who/WP heads/VBZ back/RB into/IN the/DT courtroom/NN ,/, by/IN way/NN of/IN the/DT Everglades/NNPS ,/, to/TO defend/VB a/DT young/JJ ,/, educated/VBN black/JJ man/NN -LRB-/-LRB- Blair/NNP Underwood/NNP -RRB-/-RRB- ./.\n",
            "36 NEG 14\n",
            "Among/IN multitude/NN of/IN erotic/JJ thrillers/NNS ,/, that/WDT had/VBD been/VBN released/VBN in/IN the/DT early/JJ 1990s/CD ,/, WOMAN/NNP OF/IN DESIRE/NN is/VBZ interesting/JJ only/RB because/IN it/PRP was/VBD directed/VBN by/IN Robert/NNP Ginty/NNP ,/, star/NN of/IN the/DT B-grade/JJ action/NN films/NNS of/IN the/DT previous/JJ decade/NN ./.\n",
            "37 NEG 23\n",
            "``/`` Lake/NNP Placid/NNP ''/'' marks/NNS yet/RB another/DT entry/NN in/IN the/DT series/NN of/IN ``/`` predator/NN pics/NNS ''/'' that/WDT were/VBD a/DT screen/NN staple/NN in/IN the/DT late/JJ 1970s/NNS -LRB-/-LRB- post/NN -/: ``/`` Jaws/NNP ''/'' -RRB-/-RRB- and/CC were/VBD revived/VBN recently/RB by/IN the/DT godawful/JJ ``/`` Anaconda/NNP ./. ''/''\n",
            "38 NEG 38\n",
            "CAPSULE/NNP :/: Where/WRB are/VBP you/PRP tonight/RB ,/, Leni/NNP Rienfenstal/NNP ?/.\n",
            "39 NEG 24\n",
            "Marking/VBG the/DT centennial/JJ anniversary/NN of/IN the/DT 1896/CD H.G.Wells/NNS classic/JJ ,/, New/NNP Line/NNP Cinema/NNP ,/, armed/VBN with/IN a/DT stellar/JJ cast/NN and/CC expert/NN make-up/NN effects/NNS man/NN Stan/NNP Winston/NNP -LRB-/-LRB- Alien/NNP ,/, Predator/NNP ,/, Terminator/NNP etc/NN -RRB-/-RRB- churns/VBZ out/RP yet/RB another/DT Hollywood/NNP film/NN based/VBN on/IN the/DT classic/JJ novel/NN ./.\n",
            "40 NEG 15\n",
            "Lengthy/JJ and/CC lousy/JJ are/VBP two/CD words/NNS to/TO describe/VB the/DT boring/JJ drama/NN The/DT English/NNP Patient/NN ./.\n",
            "41 NEG 31\n",
            "Starring/VBG Terrence/NNP Stamp/NNP ,/, Peter/NNP Fonda/NNP ./.\n",
            "42 NEG 24\n",
            "HIGH/JJ ART/NN is/VBZ n't/RB ./.\n",
            "43 NEG 15\n",
            "``/`` Party/NNP Camp/NNP ,/, ''/'' is/VBZ one/CD of/IN the/DT most/RBS mindnumbingly/RB brainless/JJ comedies/NNS I/PRP 've/VBP seen/VBN in/IN awhile/RB ./.\n",
            "44 NEG 32\n",
            "I/PRP 'm/VBP currently/RB accepting/VBG all/DT future/JJ names/NNS for/IN Drew/NNP Barrymore/NNP characters/NNS ./.\n",
            "45 NEG 36\n",
            "When/WRB it/PRP comes/VBZ to/TO the/DT average/JJ teenage/JJ romantic/JJ comedy/NN ,/, I/PRP expect/VBP negative/JJ reviews/NNS from/IN critics/NNS left/VBD and/CC right/NN ./.\n",
            "46 NEG 39\n",
            "All/DT through/IN its/PRP$ production/NN and/CC into/IN the/DT early/JJ days/NNS of/IN its/PRP$ initial/JJ ,/, aborted/JJ pre-release/JJ publicity/NN ,/, HARD/JJ RAIN/NN bore/VBD the/DT appropriate/JJ moniker/NN of/IN THE/DT FLOOD/NN ./.\n",
            "47 NEG 37\n",
            "Not/RB far/RB from/IN here/RB there/EX 's/VBZ a/DT fence/NN ./.\n",
            "48 NEG 21\n",
            "Sydney/NNP Lumet/NNP is/VBZ the/DT director/NN whose/WP$ work/NN happens/VBZ to/TO be/VB of/IN varied/JJ quality/NN ./.\n",
            "49 NEG 40\n",
            "``/`` The/DT world/NN on/IN land/NN --/: it/PRP 's/VBZ just/RB too/RB big/JJ for/IN me/PRP ./. ''/''\n",
            "50 NEG 30\n",
            "MY/NNP GIANT/NNP is/VBZ two/CD movies/NNS for/IN the/DT price/NN of/IN one/CD ,/, but/CC neither/DT is/VBZ worth/IN the/DT cost/NN of/IN admission/NN ,/, even/RB if/IN you/PRP get/VBP in/IN free/JJ ./.\n",
            "51 NEG 20\n",
            "Talk/NN about/IN a/DT movie/NN that/WDT seemed/VBD dated/VBN before/IN it/PRP even/RB hit/VBD the/DT theaters/NNS !/.\n",
            "52 NEG 24\n",
            "SUMMER/NN CATCH/NN Summer/NN Catch/NN is/VBZ a/DT minor/JJ league/NN effort/NN ,/, nine/CD innings/NNS of/IN banality/NN with/IN a/DT lineup/NN of/IN stock/NN situations/NNS and/CC stereotypical/JJ characters/NNS ./.\n",
            "53 NEG 73\n",
            "One/CD of/IN the/DT responses/NNS those/DT that/WDT enjoy/VBP ``/`` Detroit/NNP Rock/NNP City/NNP ''/'' -LRB-/-LRB- probably/RB KISS/NN fans/NNS ,/, mostly/RB -RRB-/-RRB- might/MD have/VB upon/IN first/JJ glance/NN at/IN the/DT rating/NN I/PRP 've/VBP given/VBN the/DT film/NN might/MD be/VB something/NN like/IN :/: ``/`` Oh/UH ,/, that/IN Casey/NNP 's/POS gone/VBN and/CC become/VB a/DT jaded/JJ critic/NN on/IN us/PRP ./. ''/''\n",
            "54 NEG 45\n",
            "Has/VBZ it/PRP really/RB been/VBN two/CD decades/NNS since/IN Walter/NNP Matthau/NNP coached/VBD THE/DT BAD/JJ NEWS/NN BEARS/NNS ?/.\n",
            "55 NEG 26\n",
            "Funny/JJ how/WRB your/PRP$ expectations/NNS can/MD be/VB defeated/VBN ,/, and/CC not/RB in/IN good/JJ ways/NNS ./.\n",
            "56 NEG 27\n",
            "Unfortunately/RB it/PRP does/VBZ n't/RB get/VB much/RB more/RBR formulaic/JJ than/IN One/CD Tough/JJ Cop/NN ./.\n",
            "57 NEG 16\n",
            "Supposedly/RB based/VBN on/IN a/DT true/JJ story/NN in/IN which/WDT the/DT British/JJ drive/NN to/TO build/VB a/DT rail/NN bridge/NN deep/RB in/IN Africa/NNP grinds/VBZ to/TO a/DT halt/NN after/IN a/DT pair/NN of/IN lions/NNS start/VBP killing/VBG off/RP the/DT workers/NNS in/IN 1898/CD ./.\n",
            "58 NEG 14\n",
            "PICTURE/NN PERFECT/NN --/: a/DT film/NN review/NN by/IN Justin/NNP K./NNP Siegel/NNP Of/IN course/NN I/PRP knew/VBD this/DT going/VBG in/IN ./.\n",
            "59 NEG 14\n",
            "Louie/NNP is/VBZ a/DT trumpeter/NN swan/NN with/IN no/DT voice/NN ./.\n",
            "60 NEG 28\n",
            "One/CD of/IN the/DT most/RBS respected/JJ names/NNS in/IN American/JJ independent/JJ filmmaking/NN is/VBZ John/NNP Sayles/NNP ./.\n",
            "61 NEG 69\n",
            "Okay/UH ,/, here/RB 's/VBZ the/DT deal/NN ./.\n",
            "62 NEG 29\n",
            "There/EX are/VBP two/CD things/NNS the/DT American/JJ film/NN industry/NN should/MD avoid/VB at/IN all/DT costs/NNS ./.\n",
            "63 NEG 34\n",
            "Would/MD you/PRP believe/VB --/: in/IN real/JJ life/NN ,/, I/PRP mean/VBP --/: that/IN if/IN you/PRP were/VBD Julia/NNP Roberts/NNP ,/, that/IN you/PRP 'd/MD be/VB the/DT ugly/JJ underdog/NN to/TO your/PRP$ sister/NN ,/, the/DT creepy/JJ Catherine/NNP Zeta-Jones/NNP ?/.\n",
            "64 NEG 26\n",
            "I/PRP 've/VBP never/RB fully/RB understood/VBN Wesley/NNP Snipes/NNP 's/POS career/NN ./.\n",
            "65 NEG 59\n",
            "Movie/NN reviewers/NNS have/VBP an/DT obligation/NN to/TO see/VB the/DT good/JJ ,/, the/DT bad/JJ ,/, and/CC the/DT despicable/JJ ./.\n",
            "66 NEG 33\n",
            "Mr./NNP Nice/NNP Guy/NNP is/VBZ the/DT latest/JJS Jackie/NNP Chan/NNP film/NN ,/, so/RB you/PRP should/MD know/VB what/WP to/TO expect/VB ./.\n",
            "67 NEG 40\n",
            "``/`` First/JJ rule/NN of/IN Fight/NN Club/NNP is/VBZ ,/, do/VBP n't/RB talk/VB about/IN fight/NN club/NN ./. ''/''\n",
            "68 NEG 19\n",
            "HOME/NNP FRIES/NNP Note/VB to/TO screenwriters/NNS and/CC self/NN :/: when/WRB you/PRP hit/VBD the/DT big/JJ time/NN ,/, and/CC the/DT studios/NNS come/VBP knocking/VBG for/IN those/DT scripts/NNS that/WDT are/VBP sitting/VBG in/IN your/PRP$ bottom/NN drawer/NN ,/, tell/VBP em/NN all/DT to/TO hit/VB the/DT road/NN ./.\n",
            "69 NEG 34\n",
            "_________________________________________________________/NN The/DT recent/JJ onslaught/NN of/IN film/NN noir/JJ that/WDT has/VBZ popped/VBN up/RP in/IN multiplexes/NNS ,/, with/IN everything/NN ranging/VBG from/IN ``/`` L.A./NNP Confidential/NNP ''/'' to/TO ``/`` Palmetto/NNP ''/'' to/TO ``/`` The/DT Big/JJ Lebowski/NNP ,/, ''/'' has/VBZ proved/VBN to/TO be/VB an/DT artistic/JJ commodity/NN for/IN veteran/NN ,/, talented/JJ directors/NNS ./.\n",
            "70 NEG 44\n",
            "ARMAGEDDON/NNP It/PRP rocks-actually/RB ,/, lots/NNS of/IN rocks/NNS fly/VBP at/IN us/PRP or/CC from/IN us/PRP ,/, in/IN slow/JJ or/CC fast/JJ motion/NN ,/, at/IN several/JJ points/NNS in/IN the/DT film/NN ./.\n",
            "71 NEG 34\n",
            "There/EX 's/VBZ a/DT 1,000-foot/JJ tidal/JJ wave/NN at/IN the/DT end/NN of/IN DEEP/JJ IMPACT/NN ./.\n",
            "72 NEG 32\n",
            "``/`` There/EX will/MD be/VB another/DT ,/, ''/'' the/DT ads/NNS for/IN this/DT sequel/NN proclaimed/VBD ,/, and/CC why/WRB not/RB ?/.\n",
            "73 NEG 31\n",
            "Late/RB in/IN Down/NN To/TO You/PRP ,/, the/DT lead/NN female/JJ character/NN -LRB-/-LRB- Julia/NNP Stiles/NNP -RRB-/-RRB- states/VBZ her/PRP$ greatest/JJS fear/NN is/VBZ having/VBG an/DT ``/`` artificial/JJ conversation/NN ''/'' with/IN her/PRP$ boyfriend/NN -LRB-/-LRB- Freddie/NNP Prinze/NNP Jr./NNP -RRB-/-RRB- ./.\n",
            "74 NEG 28\n",
            "For/IN better/JJR or/CC worse/JJR ,/, the/DT appearance/NN of/IN BASIC/NNP INSTINCT/NNP in/IN the/DT movie/NN marketplace/NN gave/VBD the/DT greenlight/NN to/TO a/DT whole/JJ slew/NN of/IN overheated/VBN ,/, oversexed/VBN ,/, underwritten/JJ thrillers/NNS ./.\n",
            "75 NEG 19\n",
            "It/PRP was/VBD with/IN great/JJ anticipation/NN that/IN I/PRP sat/VBD down/IN to/TO view/VB BRAVEHEART/NNP last/JJ week/NN as/IN it/PRP premiered/VBD on/IN American/NNP cable/NN ./.\n",
            "76 NEG 12\n",
            "Susan/NNP Granger/NNP 's/POS review/NN of/IN ``/`` THE/DT WATCHER/NN ''/'' -LRB-/-LRB- Universal/NNP -RRB-/-RRB- Just/RB what/WP we/PRP need/VBP :/: another/DT lurid/NN ,/, trashy/JJ serial/JJ killer/NN saga/NN ./.\n",
            "77 NEG 33\n",
            "Ahh/NNP yes/RB ./.\n",
            "78 NEG 20\n",
            "``/`` Virus/NN ''/'' is/VBZ the/DT type/NN of/IN cliched/JJ ,/, vacuous/JJ film/NN that/WDT has/VBZ been/VBN recylcled/VBN so/RB many/JJ times/NNS before/IN that/IN you/PRP wonder/VBP why/WRB anyone/NN would/MD even/RB bother/VB putting/VBG work/NN into/IN making/VBG it/PRP ./.\n",
            "79 NEG 50\n",
            "Over/IN 40/CD years/NNS ago/RB ,/, a/DT Japanese/JJ production/NN company/NN called/VBD Toho/NNP introduced/VBD the/DT Land/NN of/IN the/DT Rising/VBG Sun/NNP to/TO Gojira/NNP ,/, a/DT reptilian/JJ creature/NN of/IN immense/JJ proportions/NNS created/VBN by/IN mankind/NN 's/POS nuclear/JJ testing/NN ./.\n",
            "80 NEG 27\n",
            "Before/IN the/DT remake/NN of/IN Psycho/NN appears/VBZ ,/, we/PRP 've/VBP got/VBN to/TO suffer/VB through/IN this/DT remake/NN of/IN an/DT earlier/JJR Hitchcock/NNP film/NN ,/, Dial/JJ M/NN For/IN Murder/NN ./.\n",
            "81 NEG 12\n",
            "Coinciding/VBG with/IN the/DT emerging/VBG popularity/NN of/IN movies/NNS that/WDT deal/VBP with/IN anything/NN related/JJ to/TO serial/JJ killers/NNS ,/, RELENTLESS/NNP ,/, 1989/CD low/JJ budget/NN thriller/NN about/IN LAPD/NN detective/NN Sam/NNP Dietz/NNP ,/, played/VBN by/IN character/NN actor/NN Leo/NNP Rossi/NNP ,/, spawned/VBD three/CD more/JJR sequels/NNS ./.\n",
            "82 NEG 26\n",
            "Edward/NNP Burns/NNP tackles/VBZ his/PRP$ third/JJ picture/NN with/IN No/DT Looking/VBG Back/RB ,/, and/CC like/IN his/PRP$ previous/JJ two/CD ,/, it/PRP is/VBZ a/DT working-class/JJ relationship/NN picture/NN ./.\n",
            "83 NEG 37\n",
            "I/PRP do/VBP n't/RB expect/VB much/JJ from/IN Eddie/NNP Murphy/NNP these/DT days/NNS ./.\n",
            "84 NEG 25\n",
            "Brian/NNP De/NNP Palma/NNP ,/, the/DT director/NN who/WP bought/VBD us/PRP Carrie/NNP ,/, Dressed/VBN To/TO Kill/VB and/CC Mission/NNP :/: Impossible/JJ is/VBZ back/RB ,/, and/CC has/VBZ bought/VBN all/PDT his/PRP$ technical/JJ expertise/NN with/IN him/PRP ./.\n",
            "85 NEG 37\n",
            "It/PRP 's/VBZ now/RB the/DT anniversary/NN of/IN the/DT slayings/NNS of/IN Julie/NNP James/NNP '/POS -LRB-/-LRB- Jennifer/NNP Love/NNP Hewitt/NNP -RRB-/-RRB- best/JJS friends/NNS ./.\n",
            "86 NEG 44\n",
            "Lucas/NNP was/VBD wise/JJ to/TO start/VB his/PRP$ Star/NNP Wars/NNP trilogy/NN with/IN Episode/NN 4/CD :/: Episode/NN 1/CD is/VBZ a/DT boring/NN ,/, empty/JJ spectacle/NN that/WDT features/VBZ some/DT nice/JJ special/JJ effects/NNS ./.\n",
            "87 NEG 43\n",
            "Five/CD girls/NNS spend/VBP a/DT day/NN in/IN a/DT closed/JJ building/NN doing/VBG inventory/NN when/WRB a/DT strange/JJ box/NN gets/VBZ delivered/VBN there/RB -LRB-/-LRB- is/VBZ this/DT starting/VBG to/TO sound/VB familiar/JJ ?/. -RRB-/-RRB-\n",
            "88 NEG 21\n",
            "The/DT first/JJ film/NN produced/VBN by/IN Adam/NNP Sandler/NNP 's/POS Happy/JJ Madison/NNP production/NN company/NN -LRB-/-LRB- clever/JJ title/NN eh/UH ?/. -RRB-/-RRB-\n",
            "89 NEG 22\n",
            "TECTONIC/NNP PLATES/NNP is/VBZ an/DT incredibly/RB painful/JJ motion/NN picture/NN experience/NN that/WDT nearly/RB prompted/VBD me/PRP to/TO walk/VB out/RB 20/CD minutes/NNS into/IN the/DT movie/NN ./.\n",
            "90 NEG 46\n",
            "A/DT recently/RB uncovered/VBN one-fourth/NN of/IN Orson/NNP Welle/NNP 's/POS planned/VBN ``/`` docu-drama/NN ''/'' of/IN amazing/JJ stories/NNS around/IN the/DT world/NN named/VBN IT/PRP 'S/VBZ ALL/NN TRUE/JJ ,/, this/DT short/JJ dealt/VBN with/IN Welle/NNP 's/POS attempts/NNS to/TO document/VB the/DT actions/NNS of/IN several/JJ Brazilian/JJ fisherman/NN in/IN 1942/CD who/WP had/VBD become/VBN folk/JJ heroes/NNS for/IN their/PRP$ fantastic/JJ voyage/NN to/TO Rio/NNP on/IN a/DT raft/NN ./.\n",
            "91 NEG 19\n",
            "CAPSULE/NNP :/: The/DT weakest/JJS and/CC least/JJS engaging/JJ of/IN the/DT Alien/JJ movies/NNS ,/, dragged/VBD down/RP by/IN an/DT uninvolving/JJ story/NN and/CC no/DT real/JJ tension/NN ./.\n",
            "92 NEG 23\n",
            "Monkeybone/NNP ``/`` From/IN the/DT director/NN of/IN ''/'' The/DT Nightmare/NN Before/IN Christmas/NNP ``/`` it/PRP said/VBD in/IN the/DT previews/NNS ,/, which/WDT I/PRP 'm/VBP guessing/VBG was/VBD said/VBN to/TO get/VB people/NNS to/TO go/VB see/VB the/DT movie/NN ,/, obviously/RB it/PRP has/VBZ n't/RB worked/VBN since/IN the/DT $/$ 75/CD million/CD dollar/NN movie/NN has/VBZ yet/RB to/TO break/VB $/$ 5/CD million/CD ./. ''/''\n",
            "93 NEG 12\n",
            "Starring/VBG :/: Kurt/NNP Russell/NNP ,/, Jason/NNP Scott/NNP Lee/NNP ,/, Connie/NNP Nielsen/NNP ,/, Jason/NNP Isaacs/NNP ,/, Gary/NNP Busey/NNP ,/, Sean/NNP Pertwee/NNP ./.\n",
            "94 NEG 29\n",
            "AMERICAN/NNP PIMP/NN -LRB-/-LRB- director/writer/NN :/: Allen/NNP and/CC Albert/NNP Hughes/NNP ;/: cinematographer/NN :/: Albert/NNP Hughes/NNP ;/: editor/NN :/: Doug/NNP Ray/NNP ;/: cast/NN :/: Rosebudd/NNP ,/, C-Note/NNP ,/, Charm/NNP ,/, Gorgeous/NNP Dre/NNP ,/, K-Red/NNP ,/, Too/NNP Short/NNP ,/, Jade/NNP ,/, Latrice/NNP ,/, Spicy/NNP ,/, Filmore/NNP Slim/NNP ,/, Danny/NNP Brown/NNP ,/, Bishop/NNP Don/NNP Magic/NNP Juan/NNP ;/: Runtime/NNP :/: 86/CD ;/: MGM/Underworld/NNP Entertainment/Seventh/NNP Art/NNP release/NN ;/: 1999/CD -RRB-/-RRB- A/DT documentary/NN from/IN the/DT twin/JJ Hughes/NNP brothers/NNS ,/, Allen/NNP and/CC Albert/NNP -LRB-/-LRB- Dead/NNP Presidents/NNPS ,/, Menace/NN II/CD Society/NNP -RRB-/-RRB- ,/, about/IN street/NN pimps/NNS ,/, all/DT of/IN whom/WP are/VBP African-American/JJ ./.\n",
            "95 NEG 15\n",
            "Gord/NNP Brody/NNP -LRB-/-LRB- Tom/NNP Green/NNP -RRB-/-RRB- is/VBZ an/DT aspiring/JJ animator/NN approaching/VBG thirty/CD who/WP still/RB lives/VBZ in/IN parents/NNS Jim/NNP -LRB-/-LRB- Rip/NNP Torn/NNP -RRB-/-RRB- and/CC Julie/NNP 's/POS -LRB-/-LRB- Julie/NNP Hagerty/NNP ,/, ``/`` Airplane/NN ''/'' -RRB-/-RRB- basement/NN ./.\n",
            "96 NEG 43\n",
            "Much/JJ ado/NN about/IN nothing/NN ./.\n",
            "97 NEG 33\n",
            "SAVING/VBG GRACE/NN -LRB-/-LRB- director/NN :/: Nigel/NNP Cole/NNP ;/: screenwriter/NN :/: Craig/NNP Ferguson/Mark/NNP Crowdy/based/VBD on/IN a/DT story/NN by/IN Crowdy/NNP ;/: cinematographer/NN :/: John/NNP de/NNP Borman/NNP ;/: editor/NN :/: Alan/NNP Strachan/NNP ;/: cast/NN :/: Brenda/NNP Blethyn/NNP -LRB-/-LRB- Grace/NNP Trevethyn/NNP -RRB-/-RRB- ,/, Craig/NNP Ferguson/NNP -LRB-/-LRB- Matthew/NNP -RRB-/-RRB- ,/, Martin/NNP Clunes/NNP -LRB-/-LRB- Dr./NNP Bamford/NNP -RRB-/-RRB- ,/, Tcheky/NNP Karyo/NNP -LRB-/-LRB- Jacques/NNP -RRB-/-RRB- ,/, Jamie/NNP Foreman/NNP -LRB-/-LRB- China/NNP -RRB-/-RRB- ,/, Bill/NNP Bailey/NNP -LRB-/-LRB- Vince/NNP ,/, low-level/JJ drug/NN dealer/NN -RRB-/-RRB- ,/, Valerie/NNP Edmond/NNP -LRB-/-LRB- Nicky/NNP -RRB-/-RRB- ,/, Tristan/NNP Sturrock/NNP -LRB-/-LRB- Harvey/NNP -RRB-/-RRB- ,/, Clive/NNP Merrison/NNP -LRB-/-LRB- Quentin/NNP -RRB-/-RRB- ,/, Leslie/NNP Phillips/NNP -LRB-/-LRB- Vicar/NNP -RRB-/-RRB- ,/, Diana/NNP Quick/NNP -LRB-/-LRB- Honey/NNP -RRB-/-RRB- ,/, Phyllida/NNP Law/NNP -LRB-/-LRB- Margaret/NNP -RRB-/-RRB- ,/, Linda/NNP Kerr/NNP Scott/NNP -LRB-/-LRB- Diana/NNP -RRB-/-RRB- ,/, Ken/NNP Campbell/NNP -LRB-/-LRB- Sgt./NNP Alfred/NNP -RRB-/-RRB- ;/: Runtime/NNP :/: 93/CD ;/: 2000-UK/CD -RRB-/-RRB- Another/DT formula/NN `/`` feel/VB good/JJ '/'' quirky/JJ comedy/NN from/IN the/DT British/JJ Isles/NNP ,/, much/RB like/IN ``/`` Waking/NNP Ned/NNP Devine/JJ ,/, ''/'' and/CC a/DT host/NN of/IN other/JJ popular/JJ low/JJ budget/NN movies/NNS that/WDT turned/VBD a/DT nifty/JJ profit/NN off/IN a/DT thin/JJ story/NN line/NN ./.\n",
            "98 NEG 22\n",
            "Starring/VBG Tony/NNP Danza/NNP ,/, Michael/NNP Madsen/NNP ``/`` Love/NNP to/TO Kill/VB ''/'' starts/VBZ off/RP aimlessly/RB and/CC gets/VBZ progressively/RB less/JJR coherent/JJ as/IN time/NN passes/VBZ ./.\n",
            "99 NEG 34\n",
            "Whether/IN you/PRP like/IN the/DT Beatles/NNS or/CC not/RB ,/, nobody/NN wants/VBZ to/TO see/VB the/DT Bee/NNP Gee/NNP 's/POS take/NN on/IN some/DT of/IN the/DT Fab/NNP Four/NNP 's/POS best/JJS known/VBN songs/NNS ./.\n",
            "100 NEG 102\n",
            "Deep/JJ Impact/NN DEEP/JJ IMPACT/NN begins/VBZ the/DT official/JJ summer/NN movie/NN season/NN ,/, and/CC it/PRP also/RB brings/VBZ back/RB memories/NNS of/IN 1997/CD ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences,\n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "def print_sentence(s):\n",
        "  print(\" \".join(s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 100:\n",
        "    break\n",
        "\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "\n",
        "\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty0Fe8j2LOkG",
        "outputId": "20f0069f-e5a7-4261-87e4-a48ac8db9d02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Two',\n",
              "  'teen',\n",
              "  'couples',\n",
              "  'go',\n",
              "  'to',\n",
              "  'a',\n",
              "  'church',\n",
              "  'party',\n",
              "  ',',\n",
              "  'drink',\n",
              "  'and',\n",
              "  'then',\n",
              "  'drive',\n",
              "  '.'],\n",
              " ['Damn', 'that', 'Y2K', 'bug', '.']]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pene = []\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "    tokens = [token[0] for token in r[\"content\"][0]]\n",
        "    sentence = \" \".join(tokens)\n",
        "    hola = sentence.split(' ')\n",
        "    pene.append(hola)\n",
        "\n",
        "    if i == 1:\n",
        "        break\n",
        "\n",
        "# Now, reviews will contain a list of strings, each representing a sentence.\n",
        "pene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HmgMqBbQDOqY"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to store word polarity information\n",
        "lexicon_dict = dict()\n",
        "\n",
        "# Open the file named \"sent_lexicon\" for reading with UTF-8 encoding\n",
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    # Iterate through each line in the file\n",
        "    for line in f:\n",
        "        # Split the line into a list of pairs\n",
        "        pairs = line.split()\n",
        "\n",
        "        # Extract the word from the line (it's found in the third position and separated by '=')\n",
        "        word = pairs[2].split('=')[1]\n",
        "\n",
        "        # Extract the polarity information (it's found in the last position and separated by '=')\n",
        "        polarity = pairs[-1].split('=')[1]\n",
        "\n",
        "        # Check the polarity value and convert it to a numeric value\n",
        "        if polarity == \"positive\":\n",
        "            polarity = 1\n",
        "        elif polarity == \"negative\":\n",
        "            polarity = -1\n",
        "        else:\n",
        "            polarity = 0  # If neither positive nor negative, assume polarity as 0 (neutral)\n",
        "\n",
        "        # Add the word and its corresponding polarity to the lexicon_dict dictionary\n",
        "        lexicon_dict[word] = polarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ohaIZNkHjA-",
        "outputId": "4c2b5dd7-dcec-4f7c-ca58-51f614ccad4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Initialize empty lists to store the token-based results, ground truth results, and an intermediate list 'pene'\n",
        "token_results = []\n",
        "ground_results = []\n",
        "\n",
        "# Iterate through each review in the 'reviews' list\n",
        "for review in reviews:\n",
        "    # Check the sentiment label of the review and convert it to binary (1 for 'POS', 0 for other sentiments)\n",
        "    if review['sentiment'] == 'POS':\n",
        "        ground_results.append(1)\n",
        "    else:\n",
        "        ground_results.append(0)\n",
        "    \n",
        "    # Initialize the sum for the current review\n",
        "    review_sum = 0\n",
        "\n",
        "    # Iterate through each sentence in the 'content' of the review\n",
        "    for sentence in review[\"content\"]:\n",
        "        # Extract tokens from the sentence\n",
        "        sentence_tokens = [token for token, _ in sentence]\n",
        "        \n",
        "        # Look up token values in the 'lexicon_dict' and replace them with 0 if not found\n",
        "        sentence_tokens_values = [lexicon_dict.get(token, 0) for token in sentence_tokens]\n",
        "        \n",
        "        # Accumulate the sum of token values for the current review\n",
        "        review_sum += sum(sentence_tokens_values)\n",
        "        \n",
        "        # Append the cumulative 'review_sum' to the 'pene' list for this review\n",
        "    \n",
        "    # Check if the cumulative sum for the review is greater than 8\n",
        "    if review_sum > 8:\n",
        "        token_results.append(1)\n",
        "    else:\n",
        "        token_results.append(0)\n",
        "\n",
        "# Calculate token-based accuracy by comparing 'token_results' with 'ground_results'\n",
        "token_accuracy = 0\n",
        "for x, y in zip(token_results, ground_results):\n",
        "    if x == y:\n",
        "        token_accuracy += 1\n",
        "\n",
        "# Calculate the overall accuracy and store it in 'accuracy_1'\n",
        "accuracy_1 = token_accuracy / len(reviews)\n",
        "\n",
        "# Print the accuracy as a percentage with two decimal places\n",
        "print(\"Accuracy: %0.2f\" % accuracy_1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zUxlGOZRRPJq"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty dictionary to store word polarity information\n",
        "lexicon_dict = {}\n",
        "\n",
        "# Open the file named \"sent_lexicon\" for reading with UTF-8 encoding\n",
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    # Iterate through each line in the file\n",
        "    for line in f:\n",
        "        # Split the line into a list of pairs based on whitespace\n",
        "        pairs = line.split()\n",
        "        \n",
        "        # Extract the word from the line (it's found in the third position and separated by '=')\n",
        "        word = pairs[2].split('=')[1]\n",
        "\n",
        "        # Extract the polarity information (it's found in the last position and separated by '=')\n",
        "        pol = pairs[-1].split('=')[1]\n",
        "\n",
        "        # Extract the subtype of the word (it's found in the first position and separated by '=')\n",
        "        subtype = pairs[0].split('=')[1]\n",
        "\n",
        "        # Initialize the polarity variable to 0 (neutral)\n",
        "        polarity = 0\n",
        "\n",
        "        # Check the polarity value and convert it to a numeric value\n",
        "        if pol == \"positive\":\n",
        "            polarity = 1\n",
        "        elif pol == \"negative\":\n",
        "            polarity = -1\n",
        "        else:\n",
        "            polarity = 0\n",
        "\n",
        "        # If the subtype is 'strongsubj', double the polarity value\n",
        "        if subtype == 'strongsubj':\n",
        "            polarity *= 2\n",
        "\n",
        "        # Add the word and its corresponding polarity to the lexicon_dict dictionary\n",
        "        lexicon_dict[word] = polarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ll7N4drUIX8",
        "outputId": "7fbaedb7-cefc-4c4e-fa98-c2ffc2b545ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty list to store the token-based results for a different set (token_results_2) \n",
        "# and a list to store the ground truth results\n",
        "token_results_2 = []\n",
        "ground_results = []\n",
        "\n",
        "# Iterate through each review in the 'reviews' list\n",
        "for review in reviews:\n",
        "    # Check the sentiment label of the review and convert it to binary (1 for 'POS', 0 for other sentiments)\n",
        "    if review['sentiment'] == 'POS':\n",
        "        ground_results.append(1)\n",
        "    else:\n",
        "        ground_results.append(0)\n",
        "    \n",
        "    # Initialize the sum for the current review\n",
        "    review_sum = 0\n",
        "\n",
        "    # Iterate through each sentence in the 'content' of the review\n",
        "    for sentence in review[\"content\"]:\n",
        "        # Extract tokens from the sentence\n",
        "        sentence_tokens = [token for token, _ in sentence]\n",
        "        \n",
        "        # Look up token values in the 'lexicon_dict' and replace them with 0 if not found\n",
        "        sentence_tokens_values = [lexicon_dict.get(token, 0) for token in sentence_tokens]\n",
        "        \n",
        "        # Accumulate the sum of token values for the current review\n",
        "        review_sum += sum(sentence_tokens_values)\n",
        "    \n",
        "    # Check if the cumulative sum for the review is greater than 8\n",
        "    if review_sum > 8:\n",
        "        token_results_2.append(1)\n",
        "    else:\n",
        "        token_results_2.append(0)\n",
        "\n",
        "# Calculate token-based accuracy by comparing 'token_results_2' with 'ground_results'\n",
        "token_accuracy = 0\n",
        "for x, y in zip(token_results_2, ground_results):\n",
        "    if x == y:\n",
        "        token_accuracy += 1\n",
        "\n",
        "# Calculate the overall accuracy for this set and store it in 'accuracy_2'\n",
        "accuracy_2 = token_accuracy / len(reviews)\n",
        "\n",
        "# Print the accuracy as a percentage with two decimal places\n",
        "print(\"Accuracy: %0.2f\" % accuracy_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "8LgBcYcXsEk3",
        "outputId": "dd06fbd9-30d7-4838-a453-68c6bbc4f3ca"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(['token', 'magnitude'],[accuracy_1, accuracy_2])\n",
        "plt.ylim([0, 1])\n",
        "# Show Plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-DKB7oh0u38",
        "outputId": "779585c7-0332-489f-a68d-0d8cc5552395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: %0.2f 0.648\n"
          ]
        }
      ],
      "source": [
        "length = []\n",
        "for review in reviews:\n",
        "    content_length = len(review[\"content\"])\n",
        "    length.append(content_length)\n",
        "\n",
        "average_len_review = np.average(length)\n",
        "\n",
        "token_results_3 = []\n",
        "\n",
        "for review in reviews:\n",
        "    review_sum = 0  # Initialize the sum for the review\n",
        "    review_len = len(review['content'])\n",
        "    for sentence in review[\"content\"]:\n",
        "        sentence_tokens = [token for token, _ in sentence]\n",
        "        sentence_tokens_values = [lexicon_dict.get(token, 0) for token in sentence_tokens]\n",
        "        review_sum += sum(sentence_tokens_values)  # Accumulate the sum for the review\n",
        "\n",
        "    avg_sentiment_word = review_sum / review_len\n",
        "\n",
        "    # Calculate the threshold based on the average review length\n",
        "    if avg_sentiment_word == 0:\n",
        "        # Apply a smoothing factor (0.1) to avoid division by zero\n",
        "        threshold = 8/(0.1 * average_len_review * review_len)\n",
        "    else:\n",
        "        threshold = 8/(avg_sentiment_word * average_len_review * review_len)\n",
        "\n",
        "    if review_sum > threshold:\n",
        "        token_results_3.append(1)\n",
        "    else:\n",
        "        token_results_3.append(0)\n",
        "\n",
        "\n",
        "token_accuracy = 0\n",
        "for x, y in zip(token_results_3, ground_results):\n",
        "  if x == y:\n",
        "    token_accuracy += 1\n",
        "\n",
        "accuracy_3 = token_accuracy/len(reviews)\n",
        "\n",
        "print(\"Accuracy: %0.2f\", accuracy_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "Q_thMm9Pjnxt"
      },
      "outputs": [],
      "source": [
        "def train(reviews):\n",
        "    # Initialize sets to store unique words in positive and negative reviews\n",
        "    voc_pos = set()\n",
        "    voc_neg = set()\n",
        "\n",
        "    # Initialize counters for positive and negative reviews\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "\n",
        "    # Dictionaries to store word frequencies in positive and negative reviews\n",
        "    positive_word_count = {}\n",
        "    negative_word_count = {}\n",
        "\n",
        "    # Iterate through the training reviews\n",
        "    for review in reviews:\n",
        "        # Check if the review sentiment is positive\n",
        "        if review['sentiment'] == 'POS':\n",
        "            pos += 1  # Increment the positive review count\n",
        "            for sentence in review['content']:\n",
        "                for word, _ in sentence:\n",
        "                    # Add words to the positive vocabulary set and update their counts\n",
        "                    if word not in voc_pos:\n",
        "                        positive_word_count[word] = 1\n",
        "                        voc_pos.add(word)\n",
        "                        #positive_word_count[word] = 1\n",
        "                    else:\n",
        "                        positive_word_count[word] += 1\n",
        "        else:\n",
        "            neg += 1  # Increment the negative review count\n",
        "            for sentence in review['content']:\n",
        "                for word, _ in sentence:\n",
        "                    # Add words to the negative vocabulary set and update their counts\n",
        "                    if word not in voc_neg:\n",
        "                        voc_neg.add(word)\n",
        "                        negative_word_count[word] = 1\n",
        "                    else:\n",
        "                        negative_word_count[word] += 1\n",
        "\n",
        "    # Calculate prior probabilities for positive and negative reviews\n",
        "    prior_positive = pos / len(reviews)\n",
        "    prior_negative = neg / len(reviews)\n",
        "\n",
        "    # Create dictionaries to store probabilities for positive and negative words\n",
        "    prob_pos = {}\n",
        "    prob_neg = {}\n",
        "\n",
        "    # Calculate probabilities for positive words using Laplace smoothing\n",
        "    P = sum(positive_word_count.values())   # Total word count in positive reviews\n",
        "    for word in voc_pos:\n",
        "        prob_pos[word] = (positive_word_count[word]) / (P)\n",
        "\n",
        "    # Calculate probabilities for negative words using Laplace smoothing\n",
        "    N = sum(negative_word_count.values())  # Total word count in negative reviews\n",
        "    for word in voc_neg:\n",
        "        prob_neg[word] = (negative_word_count[word]) / (N)\n",
        "    \n",
        "    return prior_positive, prior_negative, voc_pos, voc_neg, prob_pos, prob_neg\n",
        "\n",
        "\n",
        "## test reviews \n",
        "prior_positive, prior_negative, voc_pos, voc_neg, prob_pos, prob_neg = train(train_reviews)\n",
        "weird = []\n",
        "# def test(reviews):\n",
        "#     for review in reviews:\n",
        "#         # Check if the review sentiment is positive\n",
        "#         if review['sentiment'] == 'POS':\n",
        "#             score_pos = np.log(prior_positive)\n",
        "#             for sentence in review['content']:\n",
        "#                 for token, _ in sentence:\n",
        "#                     if token in voc_pos:\n",
        "#                         # Token found in training data, use its probability\n",
        "#                         score_pos += np.log(prob_pos[token])\n",
        "\n",
        "#         else:\n",
        "#             score_neg = np.log(prior_negative)\n",
        "#             for sentence in review['content']:\n",
        "#                 for token, _ in sentence:\n",
        "#                     if token in voc_neg:\n",
        "#                         # Token found in training data, use its probability\n",
        "#                         score_neg += np.log(prob_neg[token])\n",
        "    \n",
        "#     return np.argmax(score_pos), np.argmax(score_neg)\n",
        "\n",
        "def test(reviews, prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg):\n",
        "    predictions = []  # Store class predictions for each review\n",
        "\n",
        "    for review in reviews:\n",
        "        score_pos = np.log(prior_positive)\n",
        "        score_neg = np.log(prior_negative)\n",
        "\n",
        "        for sentence in review['content']:\n",
        "            for token, _ in sentence:\n",
        "                if token in voc_pos:\n",
        "                    # Token found in training data, use its probability for the positive class\n",
        "                    score_pos += np.log(prob_pos[token])\n",
        "                if token in voc_neg:\n",
        "                    # Token found in training data, use its probability for the negative class\n",
        "                    score_neg += np.log(prob_neg[token])\n",
        "\n",
        "        # Compare the accumulated scores to make a prediction\n",
        "        if score_pos > score_neg:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def accuracy(reviews, predictions):\n",
        "    ground_results = []\n",
        "    for review in reviews:\n",
        "        # Check the sentiment label of the review and convert it to binary (1 for 'POS', 0 for other sentiments)\n",
        "        if review['sentiment'] == 'POS':\n",
        "            ground_results.append(1)\n",
        "        else:\n",
        "            ground_results.append(0)\n",
        "    if len(ground_results) == len(predictions):\n",
        "        token_accuracy = 0\n",
        "        for x, y in zip(predictions, ground_results):\n",
        "            if x == y:\n",
        "                token_accuracy += 1\n",
        "\n",
        "        accuracy = token_accuracy/len(reviews)\n",
        "    else:\n",
        "        raise TypeError('Lists of different sizes for accuracy testing')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.455\n"
          ]
        }
      ],
      "source": [
        "train_reviews = reviews[:900] + reviews[1000:1900]\n",
        "test_reviews = reviews[900:1000] + reviews[1900:2000]\n",
        "\n",
        "prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg = train(train_reviews)\n",
        "\n",
        "predictions = test(test_reviews, prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg)\n",
        "\n",
        "accuracy(test_reviews, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "No."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.08256880733944955\n"
          ]
        }
      ],
      "source": [
        "train_reviews = reviews[:89] + reviews[1000:1900]\n",
        "test_reviews = reviews[900:909] + reviews[1900:2000]\n",
        "# NB\n",
        "prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg = train(train_reviews)\n",
        "\n",
        "predictions_NB = test(test_reviews, prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg)\n",
        "\n",
        "accuracy(test_reviews, predictions_NB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)} for a word\n",
        "<!-- $w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the accuracy.\n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(reviews):\n",
        "    # Initialize sets to store unique words in positive and negative reviews\n",
        "    voc_pos = set()\n",
        "    voc_neg = set()\n",
        "\n",
        "    # Initialize counters for positive and negative reviews\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "\n",
        "    # Dictionaries to store word frequencies in positive and negative reviews\n",
        "    positive_word_count = {}\n",
        "    negative_word_count = {}\n",
        "\n",
        "    # Iterate through the training reviews\n",
        "    for review in reviews:\n",
        "        # Check if the review sentiment is positive\n",
        "        if review['sentiment'] == 'POS':\n",
        "            pos += 1  # Increment the positive review count\n",
        "            for sentence in review['content']:\n",
        "                for word, _ in sentence:\n",
        "                    # Add words to the positive vocabulary set and update their counts\n",
        "                    if word not in voc_pos:\n",
        "                        positive_word_count[word] = 1\n",
        "                        voc_pos.add(word)\n",
        "                        #positive_word_count[word] = 1\n",
        "                    else:\n",
        "                        positive_word_count[word] += 1\n",
        "        else:\n",
        "            neg += 1  # Increment the negative review count\n",
        "            for sentence in review['content']:\n",
        "                for word, _ in sentence:\n",
        "                    # Add words to the negative vocabulary set and update their counts\n",
        "                    if word not in voc_neg:\n",
        "                        voc_neg.add(word)\n",
        "                        negative_word_count[word] = 1\n",
        "                    else:\n",
        "                        negative_word_count[word] += 1\n",
        "\n",
        "    # Calculate prior probabilities for positive and negative reviews\n",
        "    prior_positive = pos / len(reviews)\n",
        "    prior_negative = neg / len(reviews)\n",
        "\n",
        "    # Create dictionaries to store probabilities for positive and negative words\n",
        "    prob_pos = {}\n",
        "    prob_neg = {}\n",
        "\n",
        "    # Calculate probabilities for positive words using Laplace smoothing\n",
        "    P = sum(positive_word_count.values())  # Total word count in positive reviews\n",
        "    for word in voc_pos:\n",
        "        prob_pos[word] = (positive_word_count[word] + 1) / (P + len(voc_pos))\n",
        "\n",
        "    # Calculate probabilities for negative words using Laplace smoothing\n",
        "    N = sum(negative_word_count.values())  # Total word count in negative reviews\n",
        "    for word in voc_neg:\n",
        "        prob_neg[word] = (negative_word_count[word] + 1) / (N + len(voc_neg))\n",
        "    \n",
        "    return prior_positive, prior_negative, voc_pos, voc_neg, prob_pos, prob_neg\n",
        "\n",
        "def test(reviews, prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg):\n",
        "    predictions = []  # Store class predictions for each review\n",
        "\n",
        "    for review in reviews:\n",
        "        score_pos = np.log(prior_positive)\n",
        "        score_neg = np.log(prior_negative)\n",
        "\n",
        "        for sentence in review['content']:\n",
        "            for token, _ in sentence:\n",
        "                if token in voc_pos:\n",
        "                    # Token found in training data, use its probability for the positive class\n",
        "                    score_pos += np.log(prob_pos[token])\n",
        "                if token in voc_neg:\n",
        "                    # Token found in training data, use its probability for the negative class\n",
        "                    score_neg += np.log(prob_neg[token])\n",
        "\n",
        "        # Compare the accumulated scores to make a prediction\n",
        "        if score_pos > score_neg:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.445\n"
          ]
        }
      ],
      "source": [
        "train_reviews = reviews[:900] + reviews[1000:1900]\n",
        "test_reviews = reviews[900:1000] + reviews[1900:2000]\n",
        "\n",
        "prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg = train(train_reviews)\n",
        "\n",
        "predictions = test(test_reviews, prior_positive, prior_negative ,voc_pos, voc_neg, prob_pos, prob_neg)\n",
        "\n",
        "accuracy(test_reviews, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KeCGPa7Nuzx"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoBQm1KuNzNR"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxtCul1IrBi_"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYqKBOiIrInT"
      },
      "outputs": [],
      "source": [
        "# YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA3vee5-rJyy"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYuKMTOpq9jz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How does this number compare, in practice, to the number of features at (Q2.8)?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z8sAJeUrdtM"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of\n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOvjYe-t2Br6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCUPlPozCYUX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaxCVrs8pWSp"
      },
      "source": [
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Student 1 #12345\n",
        "# Student 2 #12345"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly.\n",
        "- Download your completed notebook using `File -> Download .ipynb`\n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
